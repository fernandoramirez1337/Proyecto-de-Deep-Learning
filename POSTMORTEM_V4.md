# Post-Mortem: V4.0 Focal Loss

**Fecha:** 2025-11-18
**Resultado:** ‚ùå FALL√ì - Empeor√≥ en lugar de mejorar

---

## üìä Resultados

| M√©trica | V3.7+TT (Baseline) | V4.0 Focal | Diferencia |
|---------|-------------------|------------|------------|
| Test Accuracy | **56.17%** | 53.33% | **-2.84%** ‚ùå |
| cs.AI Recall | **36.22%** | 28.00% | **-8.22%** ‚ùå |
| Gap Total | **3.83%** | 8.67% | **+4.84%** ‚ùå |

**Veredicto:** V4.0 es significativamente peor que V3.7+TT

---

## üîç An√°lisis del Fallo

### 1. **Overfitting Severo**

Evoluci√≥n por √©poca:

| Epoch | Train Acc | Val Acc | Gap | Trend |
|-------|-----------|---------|-----|-------|
| 1 | 47.42% | 54.47% | -7.05% | Normal |
| 2 | 55.98% | 51.25% | +4.73% | ‚ö†Ô∏è Invertido |
| 3 | 62.28% | **56.36%** | +5.92% | ‚ö†Ô∏è Gap crece |
| 4 | 68.03% | 53.08% | +14.95% | üî¥ Overfitting |
| 5 | 71.82% | 50.19% | +21.62% | üî¥ Severo |
| 6 | 75.22% | 46.47% | **+28.75%** | üî¥ Cr√≠tico |

**Early stopping en Epoch 6 ‚Üí Mejor modelo en Epoch 3**

**Problema:** El modelo memoriz√≥ el training set en lugar de generalizar.

### 2. **Test Set Collapse**

- Val Acc (Epoch 3): 56.36%
- Test Acc: 53.33%
- **Diferencia:** -3.03%

**Normal es ~1-2% gap. -3.03% indica que el modelo no generaliz√≥ bien.**

### 3. **cs.AI Recall Disminuy√≥**

- V3.7+TT: 36.22% cs.AI recall
- V4.0: 28.00% cs.AI recall
- **P√©rdida:** -8.22% (22.7% relativo)

**Focal Loss + Class Weights no mejor√≥ cs.AI, empeor√≥.**

---

## üß™ Causas del Fallo

### Causa Ra√≠z: **Focal Loss muy agresivo**

**Focal Loss Formula:**
```
FL(p_t) = -Œ±_t * (1 - p_t)^Œ≥ * log(p_t)
```

**Configuraci√≥n V4.0:**
- Œ≥ (gamma) = **2.0** (muy alto)
- Œ± (alpha) = **[2.0, 1.0, 1.0, 1.0]** (class weights)
- Label smoothing = 0.1

**Problema:** Focal Loss con Œ≥=2.0 **reduce dr√°sticamente** el peso de ejemplos f√°ciles:
- Si p_t = 0.9 (ejemplo f√°cil): (1 - 0.9)^2 = 0.01 ‚Üí **99% de reducci√≥n**
- Si p_t = 0.5 (ejemplo dif√≠cil): (1 - 0.5)^2 = 0.25 ‚Üí 75% de reducci√≥n

**Resultado:** El modelo solo aprende de ~1% de ejemplos f√°ciles, causando overfitting en dif√≠ciles.

### Factores Agravantes

#### 1. **Class Weights Duplicados**
- CrossEntropy ya tiene class weights: [2.0, 1.0, 1.0, 1.0]
- Focal Loss **tambi√©n** tiene alpha (class weights)
- **Efecto combinado:** cs.AI tiene peso x2 **dos veces** ‚Üí Overfitting en cs.AI

#### 2. **Batch Size Reducido**
- Original V3.7: batch_size = **12**
- V4.0: batch_size = **8** (por memoria MPS)
- **Impacto:** Menos estabilidad en gradientes ‚Üí Mayor varianza

#### 3. **Learning Rate Constante**
- V3.7: LR = 5e-5 funciona con CrossEntropy
- V4.0: LR = 5e-5 con Focal Loss
- **Problema:** Focal Loss requiere LR m√°s bajo (t√≠picamente 0.3x-0.5x del original)

---

## üîß C√≥mo Arreglarlo (Si se reintenta)

### **Opci√≥n A: Focal Loss Suave** (Œ≥ m√°s bajo)

```python
# En train_scibert_v4_focal.py, l√≠nea ~379
FOCAL_GAMMA = 1.0  # Cambiar de 2.0 a 1.0
# O incluso 0.5 (casi CrossEntropy pero con focus leve)
```

**Impacto Œ≥:**
- Œ≥ = 0: CrossEntropy est√°ndar
- Œ≥ = 0.5: Focal muy suave (+10-20% peso en dif√≠ciles)
- Œ≥ = 1.0: Focal moderado (+50% peso en dif√≠ciles)
- Œ≥ = 2.0: Focal agresivo (+300% peso en dif√≠ciles) ‚Üê **Demasiado**

### **Opci√≥n B: Sin Class Weights en Focal Loss**

```python
# Focal Loss YA tiene alpha (class weights incorporado)
# No duplicar con class_weights manual

# L√≠nea ~406
if AGGRESSIVE_CS_AI:
    class_weights = None  # Dejar que Focal Loss maneje balance
else:
    class_weights = compute_class_weights_from_dataset(...)
```

### **Opci√≥n C: Learning Rate Reducido**

```python
# L√≠nea ~368
LR = 3e-5  # Reducir de 5e-5 a 3e-5 (60% del original)
```

### **Opci√≥n D: Dropout Aumentado**

```python
# L√≠nea ~365
DROPOUT = 0.45  # Aumentar de 0.35 a 0.45 para m√°s regularizaci√≥n
```

### **Opci√≥n E: Combinaci√≥n Segura** (RECOMENDADO para V4.1)

```python
# Configuraci√≥n V4.1 - Focal Loss Conservador
FREEZE_BERT_LAYERS = 3
DROPOUT = 0.40              # +0.05 vs V3.7
BATCH_SIZE = 8              # (o 12 si hay memoria)
EPOCHS = 10
LR = 3e-5                   # -40% vs V3.7 (5e-5)
WEIGHT_DECAY = 0.015        # +50% vs V3.7 (0.01)
PATIENCE = 3

# Focal Loss CONSERVADOR
FOCAL_GAMMA = 1.0           # Mucho m√°s suave que 2.0
LABEL_SMOOTHING = 0.1
CLASS_WEIGHTS = None        # Dejar que Focal Loss maneje balance
USE_ADAPTIVE_FOCAL = True   # Gamma 3.0‚Üí1.5 adaptativo
```

---

## üìã Lecciones Aprendidas

### 1. **Focal Loss != Siempre Mejor**
- Funciona bien para: Object detection, imbalance severo (1:100+)
- **No siempre mejor** para: Text classification, imbalance moderado (1:3)

### 2. **No Combinar T√©cnicas sin Ajustar**
- V3.7: Class Weights **O** Threshold Tuning
- V4.0: Class Weights **Y** Focal Loss **Y** Threshold Tuning
- **Error:** Demasiadas t√©cnicas de balanceo acumuladas

### 3. **Hiperpar√°metros de Papers != Universales**
- Œ≥=2.0 funciona en paper original (object detection)
- **No garantiza** funcionar en clasificaci√≥n de texto

### 4. **Baseline Fuerte es Dif√≠cil de Superar**
- V3.7+TT ya est√° bien optimizado (8 versiones de ajuste)
- Mejoras marginales (+1-2%) requieren ajuste fino

---

## ‚úÖ Alternativas Que S√ç Funcionan

### **Alternativa 1: Multi-Class Threshold Tuning en V3.7** ‚≠ê‚≠ê‚≠ê

**Por qu√© funciona:**
- V3.7 ya es bueno (56.17%)
- Solo optimiza post-training (sin riesgo)
- Mejora esperada: +1-2% ‚Üí **~57-58%**

**Comando:**
```bash
python improve_v37_multiclass.py
```

**Tiempo:** 30-60 minutos

### **Alternativa 2: Ensemble V2 + V3.7** ‚≠ê‚≠ê

**Por qu√© funciona:**
- Combina fortalezas: V2 (alta acc) + V3.7 (alta cs.AI)
- Sin reentrenamiento
- Mejora esperada: +1.5-2.5% ‚Üí **~58-59%**

**Requiere:** Tener V2 entrenado

### **Alternativa 3: Data Augmentation** ‚≠ê

**Por qu√© funciona:**
- M√°s datos de cs.AI (clase minoritaria)
- Back-translation, synonym replacement
- Sin cambiar arquitectura

**Tiempo:** 2-3 horas

---

## üéØ Recomendaci√≥n Final

### **CORTO PLAZO (Hoy):**
```bash
# Probar multi-class threshold en V3.7
python improve_v37_multiclass.py
```
**Esperado:** ~57-58% accuracy (mejor que V4.0)

### **MEDIANO PLAZO (Esta semana):**
Si quieres reintentar Focal Loss:
```bash
# Editar train_scibert_v4_focal.py con configuraci√≥n V4.1
# Cambiar: FOCAL_GAMMA=1.0, LR=3e-5, CLASS_WEIGHTS=None
python train_scibert_v4_focal.py
```
**Esperado:** ~57-59% accuracy (si se ajusta bien)

### **LARGO PLAZO (Pr√≥xima iteraci√≥n):**
- Probar otros loss functions: Dice Loss, Tversky Loss
- Data augmentation para cs.AI
- Modelos m√°s grandes: RoBERTa, DeBERTa

---

## üìå Conclusi√≥n

**V4.0 fracas√≥ porque:**
1. Focal Loss Œ≥=2.0 demasiado agresivo
2. Class weights duplicados (manual + Focal alpha)
3. Learning rate muy alto para Focal Loss
4. Batch size reducido afect√≥ estabilidad

**Mejor estrategia actual:**
- ‚úÖ Usar V3.7+TT como baseline (56.17%)
- ‚úÖ Aplicar multi-class threshold tuning ‚Üí **~57-58%**
- ‚úÖ Si se reintenta Focal Loss, usar configuraci√≥n V4.1 conservadora

**NO reintentar V4.0 con misma configuraci√≥n.**

---

**Next steps:**
```bash
# 1. Rescatar lo que podamos de V4.0
python fix_v4_threshold.py

# 2. Mejor opci√≥n: Mejorar V3.7 (ya funciona)
python improve_v37_multiclass.py

# 3. Evaluar todo
python evaluate_all_improvements.py
```
