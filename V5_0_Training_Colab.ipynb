{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# V5.0 SciBERT - Cross-Attention + Back-Translation\n\nLR: 5e-5 | Weights: 2.0 | Dropout: 0.35 | Batch: 32\n\nResults: 57.01% acc, 41.89% cs.AI recall"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets scikit-learn matplotlib seaborn torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoModel, AutoTokenizer, MarianMTModel, MarianTokenizer\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} ({torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Dataset Upload"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('arxiv_papers_augmented.csv'):\n",
    "    DATA_PATH = 'arxiv_papers_augmented.csv'\n",
    "    SKIP_AUGMENTATION = True\n",
    "    print(\"Using pre-augmented dataset\")\n",
    "elif os.path.exists('arxiv_papers_raw.csv'):\n",
    "    DATA_PATH = 'arxiv_papers_raw.csv'\n",
    "    SKIP_AUGMENTATION = False\n",
    "    print(\"Will augment dataset (~30-40 min)\")\n",
    "else:\n",
    "    print(\"ERROR: No dataset found. Upload arxiv_papers_raw.csv or arxiv_papers_augmented.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Data Augmentation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackTranslationAugmenter:\n",
    "    def __init__(self, device=None):\n",
    "        self.device = device if device else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model_en_es = MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-en-es').to(self.device).eval()\n",
    "        self.tokenizer_en_es = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-es')\n",
    "        self.model_es_en = MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-es-en').to(self.device).eval()\n",
    "        self.tokenizer_es_en = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-es-en')\n",
    "\n",
    "    def translate(self, text, model, tokenizer, max_length=512):\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, max_length=max_length, num_beams=4, early_stopping=True)\n",
    "        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    def back_translate(self, text, max_length=512):\n",
    "        try:\n",
    "            spanish = self.translate(text, self.model_en_es, self.tokenizer_en_es, max_length)\n",
    "            return self.translate(spanish, self.model_es_en, self.tokenizer_es_en, max_length)\n",
    "        except:\n",
    "            return text\n",
    "\n",
    "    def augment_dataset(self, df, target_category='cs.AI', max_samples=450):\n",
    "        target_samples = df[df['category'] == target_category].copy()\n",
    "        if len(target_samples) > max_samples:\n",
    "            target_samples = target_samples.sample(n=max_samples, random_state=42)\n",
    "        \n",
    "        augmented_samples = []\n",
    "        for idx, row in tqdm(target_samples.iterrows(), total=len(target_samples), desc=\"Augmenting\"):\n",
    "            augmented_samples.append({\n",
    "                'title': row['title'],\n",
    "                'abstract': self.back_translate(row['abstract']),\n",
    "                'category': row['category']\n",
    "            })\n",
    "        \n",
    "        final_df = pd.concat([df, pd.DataFrame(augmented_samples)], ignore_index=True)\n",
    "        return final_df.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_AUGMENTATION:\n",
    "    df_raw = pd.read_csv(DATA_PATH)\n",
    "    augmenter = BackTranslationAugmenter()\n",
    "    df_augmented = augmenter.augment_dataset(df_raw, max_samples=450)\n",
    "    df_augmented.to_csv('arxiv_papers_augmented.csv', index=False)\n",
    "    DATA_PATH = 'arxiv_papers_augmented.csv'\n",
    "    del augmenter\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"Augmented: {len(df_augmented)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class CrossAttentionSciBERT(nn.Module):\n    def __init__(self, num_classes=4, dropout=0.35, freeze_bert_layers=3):\n        super().__init__()\n        self.bert = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased')\n        hidden_size = self.bert.config.hidden_size\n        \n        if freeze_bert_layers > 0:\n            for layer in self.bert.encoder.layer[:freeze_bert_layers]:\n                for param in layer.parameters():\n                    param.requires_grad = False\n        \n        self.embedding_dropout = nn.Dropout(0.1)\n        self.cross_attn_title_to_abstract = nn.MultiheadAttention(hidden_size, 8, dropout=0.1, batch_first=True)\n        self.cross_attn_abstract_to_title = nn.MultiheadAttention(hidden_size, 8, dropout=0.1, batch_first=True)\n        self.layer_norm1 = nn.LayerNorm(hidden_size)\n        self.layer_norm2 = nn.LayerNorm(hidden_size)\n        self.title_attention = nn.Linear(hidden_size, 1)\n        self.abstract_attention = nn.Linear(hidden_size, 1)\n        \n        self.fusion = nn.Sequential(\n            nn.Linear(hidden_size * 2, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(dropout),\n            nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU(), nn.Dropout(dropout),\n            nn.Linear(256, 128), nn.LayerNorm(128), nn.GELU(), nn.Dropout(dropout * 0.8)\n        )\n        self.classifier = nn.Linear(128, num_classes)\n        self._init_weights()\n    \n    def _init_weights(self):\n        for m in list(self.fusion.modules()) + [self.classifier]:\n            if isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, std=0.02)\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n    \n    def attention_pool(self, hidden_states, attention_layer, mask):\n        attention_weights = attention_layer(hidden_states).squeeze(-1)\n        if mask is not None:\n            attention_weights = attention_weights.masked_fill(mask == 0, -1e9)\n        attention_weights = torch.softmax(attention_weights, dim=1)\n        return torch.bmm(attention_weights.unsqueeze(1), hidden_states).squeeze(1), attention_weights\n    \n    def forward(self, title_input_ids, title_attention_mask, abstract_input_ids, abstract_attention_mask):\n        title_hidden = self.embedding_dropout(self.bert(title_input_ids, title_attention_mask).last_hidden_state)\n        abstract_hidden = self.embedding_dropout(self.bert(abstract_input_ids, abstract_attention_mask).last_hidden_state)\n        \n        title_enhanced, _ = self.cross_attn_title_to_abstract(\n            title_hidden, abstract_hidden, abstract_hidden, key_padding_mask=(abstract_attention_mask == 0))\n        title_enhanced = self.layer_norm1(title_hidden + title_enhanced)\n        \n        abstract_enhanced, _ = self.cross_attn_abstract_to_title(\n            abstract_hidden, title_hidden, title_hidden, key_padding_mask=(title_attention_mask == 0))\n        abstract_enhanced = self.layer_norm2(abstract_hidden + abstract_enhanced)\n        \n        title_pooled, _ = self.attention_pool(title_enhanced, self.title_attention, title_attention_mask)\n        abstract_pooled, _ = self.attention_pool(abstract_enhanced, self.abstract_attention, abstract_attention_mask)\n        \n        return self.classifier(self.fusion(torch.cat([title_pooled, abstract_pooled], dim=1)))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SciBERTDataset(Dataset):\n",
    "    def __init__(self, titles, abstracts, labels, tokenizer, max_title_len=32, max_abstract_len=128):\n",
    "        self.titles = titles\n",
    "        self.abstracts = abstracts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_title_len = max_title_len\n",
    "        self.max_abstract_len = max_abstract_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        title_enc = self.tokenizer(self.titles[idx], max_length=self.max_title_len, \n",
    "                                   padding='max_length', truncation=True, return_tensors='pt')\n",
    "        abstract_enc = self.tokenizer(self.abstracts[idx], max_length=self.max_abstract_len,\n",
    "                                      padding='max_length', truncation=True, return_tensors='pt')\n",
    "        return {\n",
    "            'title_input_ids': title_enc['input_ids'].squeeze(0),\n",
    "            'title_attention_mask': title_enc['attention_mask'].squeeze(0),\n",
    "            'abstract_input_ids': abstract_enc['input_ids'].squeeze(0),\n",
    "            'abstract_attention_mask': abstract_enc['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH)\n",
    "le = LabelEncoder()\n",
    "df['label'] = le.fit_transform(df['category'])\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "\n",
    "X, y = df[['title', 'abstract']], df['label']\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.15/(1-0.15), random_state=42, stratify=y_temp)\n",
    "\n",
    "train_dataset = SciBERTDataset(X_train['title'].tolist(), X_train['abstract'].tolist(), y_train.tolist(), tokenizer)\n",
    "val_dataset = SciBERTDataset(X_val['title'].tolist(), X_val['abstract'].tolist(), y_val.tolist(), tokenizer)\n",
    "test_dataset = SciBERTDataset(X_test['title'].tolist(), X_test['abstract'].tolist(), y_test.tolist(), tokenizer)\n",
    "\n",
    "print(f\"Train: {len(train_dataset)} | Val: {len(val_dataset)} | Test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "FREEZE_BERT_LAYERS = 3\nDROPOUT = 0.35\nBATCH_SIZE = 32\nEPOCHS = 10\nLR = 5e-5\nWEIGHT_DECAY = 0.01\nCLASS_WEIGHTS = [2.0, 1.0, 1.0, 1.0]\nPATIENCE = 3\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n\nmodel = CrossAttentionSciBERT(num_classes=4, dropout=DROPOUT, freeze_bert_layers=FREEZE_BERT_LAYERS).to(device)\n\nclass_weights_tensor = torch.FloatTensor(CLASS_WEIGHTS).to(device)\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.1, weight=class_weights_tensor)\n\nbert_params = [p for n, p in model.named_parameters() if 'bert' in n and p.requires_grad]\nclassifier_params = [p for n, p in model.named_parameters() if 'bert' not in n and p.requires_grad]\noptimizer = torch.optim.AdamW([\n    {'params': bert_params, 'lr': LR, 'weight_decay': WEIGHT_DECAY},\n    {'params': classifier_params, 'lr': LR * 5, 'weight_decay': WEIGHT_DECAY * 2}\n])\n\nnum_training_steps = len(train_loader) * EPOCHS\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_training_steps // 10, \n                                           num_training_steps=num_training_steps)\n\nprint(f\"Config: LR={LR} | Weights={CLASS_WEIGHTS} | Batch={BATCH_SIZE} | Steps={num_training_steps}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'val_f1': []}\n",
    "best_val_acc = 0\n",
    "best_model_state = None\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    all_train_preds, all_train_labels = [], []\n",
    "    \n",
    "    for batch in tqdm(train_loader, desc='Train'):\n",
    "        title_ids = batch['title_input_ids'].to(device)\n",
    "        title_mask = batch['title_attention_mask'].to(device)\n",
    "        abstract_ids = batch['abstract_input_ids'].to(device)\n",
    "        abstract_mask = batch['abstract_attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(title_ids, title_mask, abstract_ids, abstract_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        all_train_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "        all_train_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    train_acc = accuracy_score(all_train_labels, all_train_preds)\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    all_val_preds, all_val_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc='Val'):\n",
    "            title_ids = batch['title_input_ids'].to(device)\n",
    "            title_mask = batch['title_attention_mask'].to(device)\n",
    "            abstract_ids = batch['abstract_input_ids'].to(device)\n",
    "            abstract_mask = batch['abstract_attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(title_ids, title_mask, abstract_ids, abstract_mask)\n",
    "            val_loss += criterion(outputs, labels).item()\n",
    "            all_val_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "            all_val_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    val_acc = accuracy_score(all_val_labels, all_val_preds)\n",
    "    val_f1 = f1_score(all_val_labels, all_val_preds, average='weighted')\n",
    "    \n",
    "    print(f\"Train: {train_acc:.4f} | Val: {val_acc:.4f} | F1: {val_f1:.4f} | Gap: {abs(train_acc-val_acc):.4f}\")\n",
    "    \n",
    "    history['train_loss'].append(train_loss / len(train_loader))\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss / len(val_loader))\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_f1'].append(val_f1)\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_model_state = model.state_dict().copy()\n",
    "        patience_counter = 0\n",
    "        print(f\"‚úì Best: {val_acc:.4f}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\"Early stop\")\n",
    "            break\n",
    "\n",
    "model.load_state_dict(best_model_state)\n",
    "print(f\"\\nBest val: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "all_test_preds, all_test_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc='Test'):\n",
    "        title_ids = batch['title_input_ids'].to(device)\n",
    "        title_mask = batch['title_attention_mask'].to(device)\n",
    "        abstract_ids = batch['abstract_input_ids'].to(device)\n",
    "        abstract_mask = batch['abstract_attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        outputs = model(title_ids, title_mask, abstract_ids, abstract_mask)\n",
    "        all_test_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "        all_test_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "test_acc = accuracy_score(all_test_labels, all_test_preds)\n",
    "test_f1 = f1_score(all_test_labels, all_test_preds, average='weighted')\n",
    "recalls = recall_score(all_test_labels, all_test_preds, average=None)\n",
    "precisions = precision_score(all_test_labels, all_test_preds, average=None, zero_division=0)\n",
    "\n",
    "cs_ai_idx = list(le.classes_).index('cs.AI')\n",
    "cs_ai_recall = recalls[cs_ai_idx]\n",
    "\n",
    "print(f\"\\nTest Acc: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "print(f\"Test F1: {test_f1:.4f}\")\n",
    "print(f\"cs.AI Recall: {cs_ai_recall:.4f} ({cs_ai_recall*100:.2f}%)\")\n",
    "print(f\"\\n{classification_report(all_test_labels, all_test_preds, target_names=le.classes_, digits=4)}\")\n",
    "\n",
    "acc_met = test_acc >= 0.60\n",
    "cs_ai_met = cs_ai_recall > 0.30\n",
    "print(f\"\\nAcc ‚â•60%: {'‚úÖ' if acc_met else '‚ùå'} | cs.AI >30%: {'‚úÖ' if cs_ai_met else '‚ùå'}\")\n",
    "if acc_met and cs_ai_met:\n",
    "    print(\"üéâ BOTH OBJECTIVES MET!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\naxes[0].plot(history['train_loss'], label='Train', marker='o')\naxes[0].plot(history['val_loss'], label='Val', marker='o')\naxes[0].set_title('Loss')\naxes[0].legend()\naxes[0].grid(True)\n\naxes[1].plot(history['train_acc'], label='Train', marker='o')\naxes[1].plot(history['val_acc'], label='Val', marker='o')\naxes[1].set_title('Accuracy')\naxes[1].legend()\naxes[1].grid(True)\nplt.tight_layout()\nplt.savefig('v5_0_history.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(confusion_matrix(all_test_labels, all_test_preds), annot=True, fmt='d', cmap='Blues',\n            xticklabels=le.classes_, yticklabels=le.classes_)\nplt.title(f'V5.0 Confusion Matrix | Acc: {test_acc:.3f}')\nplt.ylabel('True')\nplt.xlabel('Predicted')\nplt.tight_layout()\nplt.savefig('v5_0_confusion.png', dpi=150, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save & Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "torch.save(model.state_dict(), 'best_v5_0.pth')\nwith open('label_encoder.pkl', 'wb') as f:\n    pickle.dump(le, f)\n\nimport json\nwith open('results.json', 'w') as f:\n    json.dump({\n        'test_accuracy': float(test_acc),\n        'test_f1': float(test_f1),\n        'cs_ai_recall': float(cs_ai_recall),\n        'best_val_acc': float(best_val_acc)\n    }, f, indent=2)\n\nprint(\"Saved: best_v5_0.pth, label_encoder.pkl, results.json, v5_0_history.png, v5_0_confusion.png\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from google.colab import files\nfor f in ['best_v5_0.pth', 'label_encoder.pkl', 'results.json', 'v5_0_history.png', 'v5_0_confusion.png']:\n    files.download(f)"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}