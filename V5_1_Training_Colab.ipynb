{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# V5.1 SciBERT - Stabilized Training\n",
        "\n",
        "LR: 3e-5 | Class weights: 1.4 | Dropout: 0.30 | Batch: 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets scikit-learn matplotlib seaborn torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import AutoModel, AutoTokenizer, MarianMTModel, MarianTokenizer\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.metrics import recall_score, precision_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)} ({torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Upload Dataset\n",
        "Upload `arxiv_papers_augmented.csv` or `arxiv_papers_raw.csv`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if os.path.exists('arxiv_papers_augmented.csv'):\n",
        "    DATA_PATH = 'arxiv_papers_augmented.csv'\n",
        "    SKIP_AUGMENTATION = True\n",
        "    print(\"Using pre-augmented dataset\")\n",
        "elif os.path.exists('arxiv_papers_raw.csv'):\n",
        "    DATA_PATH = 'arxiv_papers_raw.csv'\n",
        "    SKIP_AUGMENTATION = False\n",
        "    print(\"Will augment dataset (~30-40 min)\")\n",
        "else:\n",
        "    print(\"ERROR: No dataset found. Upload arxiv_papers_raw.csv or arxiv_papers_augmented.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Augmentation (skip if using pre-augmented)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BackTranslationAugmenter:\n",
        "    def __init__(self, device=None):\n",
        "        self.device = device if device else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model_en_es = MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-en-es').to(self.device).eval()\n",
        "        self.tokenizer_en_es = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-es')\n",
        "        self.model_es_en = MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-es-en').to(self.device).eval()\n",
        "        self.tokenizer_es_en = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-es-en')\n",
        "\n",
        "    def translate(self, text, model, tokenizer, max_length=512):\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
        "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(**inputs, max_length=max_length, num_beams=4, early_stopping=True)\n",
        "        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    def back_translate(self, text, max_length=512):\n",
        "        try:\n",
        "            spanish = self.translate(text, self.model_en_es, self.tokenizer_en_es, max_length)\n",
        "            return self.translate(spanish, self.model_es_en, self.tokenizer_es_en, max_length)\n",
        "        except:\n",
        "            return text\n",
        "\n",
        "    def augment_dataset(self, df, target_category='cs.AI', max_samples=450):\n",
        "        target_samples = df[df['category'] == target_category].copy()\n",
        "        if len(target_samples) > max_samples:\n",
        "            target_samples = target_samples.sample(n=max_samples, random_state=42)\n",
        "        \n",
        "        augmented_samples = []\n",
        "        for idx, row in tqdm(target_samples.iterrows(), total=len(target_samples), desc=\"Augmenting\"):\n",
        "            augmented_samples.append({\n",
        "                'title': row['title'],\n",
        "                'abstract': self.back_translate(row['abstract']),\n",
        "                'category': row['category']\n",
        "            })\n",
        "        \n",
        "        final_df = pd.concat([df, pd.DataFrame(augmented_samples)], ignore_index=True)\n",
        "        return final_df.sample(frac=1, random_state=42).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not SKIP_AUGMENTATION:\n",
        "    df_raw = pd.read_csv(DATA_PATH)\n",
        "    augmenter = BackTranslationAugmenter()\n",
        "    df_augmented = augmenter.augment_dataset(df_raw, max_samples=450)\n",
        "    df_augmented.to_csv('arxiv_papers_augmented.csv', index=False)\n",
        "    DATA_PATH = 'arxiv_papers_augmented.csv'\n",
        "    del augmenter\n",
        "    torch.cuda.empty_cache()\n",
        "    print(f\"Augmented: {len(df_augmented)} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CrossAttentionSciBERT(nn.Module):\n",
        "    def __init__(self, num_classes=4, dropout=0.30, freeze_bert_layers=3):\n",
        "        super().__init__()\n",
        "        self.bert = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased')\n",
        "        hidden_size = self.bert.config.hidden_size\n",
        "        \n",
        "        if freeze_bert_layers > 0:\n",
        "            for layer in self.bert.encoder.layer[:freeze_bert_layers]:\n",
        "                for param in layer.parameters():\n",
        "                    param.requires_grad = False\n",
        "        \n",
        "        self.embedding_dropout = nn.Dropout(0.1)\n",
        "        self.cross_attn_title_to_abstract = nn.MultiheadAttention(hidden_size, 8, dropout=0.1, batch_first=True)\n",
        "        self.cross_attn_abstract_to_title = nn.MultiheadAttention(hidden_size, 8, dropout=0.1, batch_first=True)\n",
        "        self.layer_norm1 = nn.LayerNorm(hidden_size)\n",
        "        self.layer_norm2 = nn.LayerNorm(hidden_size)\n",
        "        self.title_attention = nn.Linear(hidden_size, 1)\n",
        "        self.abstract_attention = nn.Linear(hidden_size, 1)\n",
        "        \n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Linear(hidden_size * 2, 512), nn.LayerNorm(512), nn.GELU(), nn.Dropout(dropout),\n",
        "            nn.Linear(512, 256), nn.LayerNorm(256), nn.GELU(), nn.Dropout(dropout),\n",
        "            nn.Linear(256, 128), nn.LayerNorm(128), nn.GELU(), nn.Dropout(dropout * 0.8)\n",
        "        )\n",
        "        self.classifier = nn.Linear(128, num_classes)\n",
        "        self._init_weights()\n",
        "    \n",
        "    def _init_weights(self):\n",
        "        for m in list(self.fusion.modules()) + [self.classifier]:\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, std=0.02)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "    \n",
        "    def attention_pool(self, hidden_states, attention_layer, mask):\n",
        "        attention_weights = attention_layer(hidden_states).squeeze(-1)\n",
        "        if mask is not None:\n",
        "            attention_weights = attention_weights.masked_fill(mask == 0, -1e9)\n",
        "        attention_weights = torch.softmax(attention_weights, dim=1)\n",
        "        return torch.bmm(attention_weights.unsqueeze(1), hidden_states).squeeze(1), attention_weights\n",
        "    \n",
        "    def forward(self, title_input_ids, title_attention_mask, abstract_input_ids, abstract_attention_mask):\n",
        "        title_hidden = self.embedding_dropout(self.bert(title_input_ids, title_attention_mask).last_hidden_state)\n",
        "        abstract_hidden = self.embedding_dropout(self.bert(abstract_input_ids, abstract_attention_mask).last_hidden_state)\n",
        "        \n",
        "        title_enhanced, _ = self.cross_attn_title_to_abstract(\n",
        "            title_hidden, abstract_hidden, abstract_hidden, key_padding_mask=(abstract_attention_mask == 0))\n",
        "        title_enhanced = self.layer_norm1(title_hidden + title_enhanced)\n",
        "        \n",
        "        abstract_enhanced, _ = self.cross_attn_abstract_to_title(\n",
        "            abstract_hidden, title_hidden, title_hidden, key_padding_mask=(title_attention_mask == 0))\n",
        "        abstract_enhanced = self.layer_norm2(abstract_hidden + abstract_enhanced)\n",
        "        \n",
        "        title_pooled, _ = self.attention_pool(title_enhanced, self.title_attention, title_attention_mask)\n",
        "        abstract_pooled, _ = self.attention_pool(abstract_enhanced, self.abstract_attention, abstract_attention_mask)\n",
        "        \n",
        "        return self.classifier(self.fusion(torch.cat([title_pooled, abstract_pooled], dim=1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SciBERTDataset(Dataset):\n",
        "    def __init__(self, titles, abstracts, labels, tokenizer, max_title_len=32, max_abstract_len=128):\n",
        "        self.titles = titles\n",
        "        self.abstracts = abstracts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_title_len = max_title_len\n",
        "        self.max_abstract_len = max_abstract_len\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        title_enc = self.tokenizer(self.titles[idx], max_length=self.max_title_len, \n",
        "                                   padding='max_length', truncation=True, return_tensors='pt')\n",
        "        abstract_enc = self.tokenizer(self.abstracts[idx], max_length=self.max_abstract_len,\n",
        "                                      padding='max_length', truncation=True, return_tensors='pt')\n",
        "        return {\n",
        "            'title_input_ids': title_enc['input_ids'].squeeze(0),\n",
        "            'title_attention_mask': title_enc['attention_mask'].squeeze(0),\n",
        "            'abstract_input_ids': abstract_enc['input_ids'].squeeze(0),\n",
        "            'abstract_attention_mask': abstract_enc['attention_mask'].squeeze(0),\n",
        "            'label': torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(DATA_PATH)\n",
        "le = LabelEncoder()\n",
        "df['label'] = le.fit_transform(df['category'])\n",
        "tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
        "\n",
        "X, y = df[['title', 'abstract']], df['label']\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.15/(1-0.15), random_state=42, stratify=y_temp)\n",
        "\n",
        "train_dataset = SciBERTDataset(X_train['title'].tolist(), X_train['abstract'].tolist(), y_train.tolist(), tokenizer)\n",
        "val_dataset = SciBERTDataset(X_val['title'].tolist(), X_val['abstract'].tolist(), y_val.tolist(), tokenizer)\n",
        "test_dataset = SciBERTDataset(X_test['title'].tolist(), X_test['abstract'].tolist(), y_test.tolist(), tokenizer)\n",
        "\n",
        "print(f\"Train: {len(train_dataset)} | Val: {len(val_dataset)} | Test: {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "FREEZE_BERT_LAYERS = 3\n",
        "DROPOUT = 0.30\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10\n",
        "LR = 3e-5\n",
        "WEIGHT_DECAY = 0.01\n",
        "CLASS_WEIGHTS = [1.4, 1.0, 1.0, 1.0]\n",
        "PATIENCE = 3\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "model = CrossAttentionSciBERT(num_classes=4, dropout=DROPOUT, freeze_bert_layers=FREEZE_BERT_LAYERS).to(device)\n",
        "\n",
        "class_weights_tensor = torch.FloatTensor(CLASS_WEIGHTS).to(device)\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1, weight=class_weights_tensor)\n",
        "\n",
        "bert_params = [p for n, p in model.named_parameters() if 'bert' in n and p.requires_grad]\n",
        "classifier_params = [p for n, p in model.named_parameters() if 'bert' not in n and p.requires_grad]\n",
        "optimizer = torch.optim.AdamW([\n",
        "    {'params': bert_params, 'lr': LR, 'weight_decay': WEIGHT_DECAY},\n",
        "    {'params': classifier_params, 'lr': LR * 5, 'weight_decay': WEIGHT_DECAY * 2}\n",
        "])\n",
        "\n",
        "num_training_steps = len(train_loader) * EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_training_steps // 10, \n",
        "                                           num_training_steps=num_training_steps)\n",
        "\n",
        "print(f\"Config: LR={LR} | Weights={CLASS_WEIGHTS} | Batch={BATCH_SIZE} | Steps={num_training_steps}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'val_f1': []}\n",
        "best_val_acc = 0\n",
        "best_model_state = None\n",
        "patience_counter = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
        "    \n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    all_train_preds, all_train_labels = [], []\n",
        "    \n",
        "    for batch in tqdm(train_loader, desc='Train'):\n",
        "        title_ids = batch['title_input_ids'].to(device)\n",
        "        title_mask = batch['title_attention_mask'].to(device)\n",
        "        abstract_ids = batch['abstract_input_ids'].to(device)\n",
        "        abstract_mask = batch['abstract_attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(title_ids, title_mask, abstract_ids, abstract_mask)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        \n",
        "        train_loss += loss.item()\n",
        "        all_train_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
        "        all_train_labels.extend(labels.cpu().numpy())\n",
        "    \n",
        "    train_acc = accuracy_score(all_train_labels, all_train_preds)\n",
        "    \n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    all_val_preds, all_val_labels = [], []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc='Val'):\n",
        "            title_ids = batch['title_input_ids'].to(device)\n",
        "            title_mask = batch['title_attention_mask'].to(device)\n",
        "            abstract_ids = batch['abstract_input_ids'].to(device)\n",
        "            abstract_mask = batch['abstract_attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "            \n",
        "            outputs = model(title_ids, title_mask, abstract_ids, abstract_mask)\n",
        "            val_loss += criterion(outputs, labels).item()\n",
        "            all_val_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
        "            all_val_labels.extend(labels.cpu().numpy())\n",
        "    \n",
        "    val_acc = accuracy_score(all_val_labels, all_val_preds)\n",
        "    val_f1 = f1_score(all_val_labels, all_val_preds, average='weighted')\n",
        "    \n",
        "    print(f\"Train: {train_acc:.4f} | Val: {val_acc:.4f} | F1: {val_f1:.4f} | Gap: {abs(train_acc-val_acc):.4f}\")\n",
        "    \n",
        "    history['train_loss'].append(train_loss / len(train_loader))\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['val_loss'].append(val_loss / len(val_loader))\n",
        "    history['val_acc'].append(val_acc)\n",
        "    history['val_f1'].append(val_f1)\n",
        "    \n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_model_state = model.state_dict().copy()\n",
        "        patience_counter = 0\n",
        "        print(f\"‚úì Best: {val_acc:.4f}\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= PATIENCE:\n",
        "            print(f\"Early stop\")\n",
        "            break\n",
        "\n",
        "model.load_state_dict(best_model_state)\n",
        "print(f\"\\nBest val: {best_val_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.eval()\n",
        "all_test_preds, all_test_labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_loader, desc='Test'):\n",
        "        title_ids = batch['title_input_ids'].to(device)\n",
        "        title_mask = batch['title_attention_mask'].to(device)\n",
        "        abstract_ids = batch['abstract_input_ids'].to(device)\n",
        "        abstract_mask = batch['abstract_attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "        \n",
        "        outputs = model(title_ids, title_mask, abstract_ids, abstract_mask)\n",
        "        all_test_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
        "        all_test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "test_acc = accuracy_score(all_test_labels, all_test_preds)\n",
        "test_f1 = f1_score(all_test_labels, all_test_preds, average='weighted')\n",
        "recalls = recall_score(all_test_labels, all_test_preds, average=None)\n",
        "precisions = precision_score(all_test_labels, all_test_preds, average=None, zero_division=0)\n",
        "\n",
        "cs_ai_idx = list(le.classes_).index('cs.AI')\n",
        "cs_ai_recall = recalls[cs_ai_idx]\n",
        "\n",
        "print(f\"\\nTest Acc: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
        "print(f\"Test F1: {test_f1:.4f}\")\n",
        "print(f\"cs.AI Recall: {cs_ai_recall:.4f} ({cs_ai_recall*100:.2f}%)\")\n",
        "print(f\"\\n{classification_report(all_test_labels, all_test_preds, target_names=le.classes_, digits=4)}\")\n",
        "\n",
        "acc_met = test_acc >= 0.60\n",
        "cs_ai_met = cs_ai_recall > 0.30\n",
        "print(f\"\\nAcc ‚â•60%: {'‚úÖ' if acc_met else '‚ùå'} | cs.AI >30%: {'‚úÖ' if cs_ai_met else '‚ùå'}\")\n",
        "if acc_met and cs_ai_met:\n",
        "    print(\"üéâ BOTH OBJECTIVES MET!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "axes[0].plot(history['train_loss'], label='Train', marker='o')\n",
        "axes[0].plot(history['val_loss'], label='Val', marker='o')\n",
        "axes[0].set_title('Loss')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True)\n",
        "\n",
        "axes[1].plot(history['train_acc'], label='Train', marker='o')\n",
        "axes[1].plot(history['val_acc'], label='Val', marker='o')\n",
        "axes[1].set_title('Accuracy')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig('v5_1_history.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(confusion_matrix(all_test_labels, all_test_preds), annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=le.classes_, yticklabels=le.classes_)\n",
        "plt.title(f'V5.1 Confusion Matrix | Acc: {test_acc:.3f}')\n",
        "plt.ylabel('True')\n",
        "plt.xlabel('Predicted')\n",
        "plt.tight_layout()\n",
        "plt.savefig('v5_1_confusion.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save & Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'best_v5_1.pth')\n",
        "with open('label_encoder.pkl', 'wb') as f:\n",
        "    pickle.dump(le, f)\n",
        "\n",
        "import json\n",
        "with open('results.json', 'w') as f:\n",
        "    json.dump({\n",
        "        'test_accuracy': float(test_acc),\n",
        "        'test_f1': float(test_f1),\n",
        "        'cs_ai_recall': float(cs_ai_recall),\n",
        "        'best_val_acc': float(best_val_acc)\n",
        "    }, f, indent=2)\n",
        "\n",
        "print(\"Saved: best_v5_1.pth, label_encoder.pkl, results.json, v5_1_history.png, v5_1_confusion.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "for f in ['best_v5_1.pth', 'label_encoder.pkl', 'results.json', 'v5_1_history.png', 'v5_1_confusion.png']:\n",
        "    files.download(f)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
